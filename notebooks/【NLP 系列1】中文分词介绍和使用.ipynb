{"cells":[{"cell_type":"markdown","metadata":{"trusted":true,"id":"78BAB78A54534A3481038366F359F0BE","scrolled":false,"mdEditEnable":false,"tags":[],"slideshow":{"slide_type":"slide"},"jupyter":{},"runtime":{"status":"default","execution_status":null,"is_visible":false},"notebookId":"666709baebd74c5c87d68191"},"source":"## 往期文章学习  \nhttps://www.kesci.com/home/column/5cb43d67e0ad99002cad14d6  "},{"cell_type":"markdown","metadata":{"id":"D179CDF36D794EBD86F0AA976AD072AB","mdEditEnable":false,"tags":[],"slideshow":{"slide_type":"slide"},"jupyter":{},"runtime":{"status":"default","execution_status":null,"is_visible":false},"scrolled":false,"notebookId":"666709baebd74c5c87d68191"},"source":"### 本篇目录  \n* 前言  \n* 安装模块  \n* 数据集介绍  \n* 1、jieba 模块功能概要  \n* 2、jieba 模块实战  \n* 3、textrank4zh 模块实战  \n* 4、snownlp 分词模块功能概要  \n* 5、参考文献  \n* -------------------  \n### 从这个项目中，你能学到哪些技术知识？  \n* jieba: 分词、提取关键词、词性标注  \n* textrank4zh: 抽取关键词、关键短语和文本摘要  \n* snownlp: 分词、汉字转拼音、繁体转简体、分句、摘要、关键词、相似度分析、情感分析、词性标注，tf和idf  \n"},{"cell_type":"markdown","metadata":{"id":"F34F58352B53446387AB1F5E780367CD","mdEditEnable":false,"tags":[],"slideshow":{"slide_type":"slide"},"jupyter":{},"runtime":{"status":"default","execution_status":null,"is_visible":false},"scrolled":false,"notebookId":"666709baebd74c5c87d68191"},"source":"### 前言"},{"cell_type":"markdown","metadata":{"id":"466CEB10AEC248A7875C1F17E0441321","mdEditEnable":false,"tags":[],"slideshow":{"slide_type":"slide"},"jupyter":{},"runtime":{"status":"default","execution_status":null,"is_visible":false},"scrolled":false,"notebookId":"666709baebd74c5c87d68191"},"source":"分词是自然语言中最基础，也是必要环节来的。  \n无论是提取关键词、相似度分析、词性标注或是以后运用算法进行训练预测等这些都是建立在文本分词的基础上。  \n文本型和数值型数据是有区别的，如果不对文本进行分词切割，就不能很好地统计和识别文本的具体类别和具体含义  。  \n文本型数据与数值型数据的区别和联系，等到下一节的'文本预处理'中再详细介绍  \n"},{"cell_type":"markdown","metadata":{"id":"4E15700113654E4AACD5963B81710841","mdEditEnable":false,"tags":[],"slideshow":{"slide_type":"slide"},"jupyter":{},"runtime":{"status":"default","execution_status":null,"is_visible":false},"scrolled":false,"notebookId":"666709baebd74c5c87d68191"},"source":"### 安装模块"},{"cell_type":"code","metadata":{"id":"F4BB2A90DF3B495A83F2B5E1766091FF","collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"jupyter":{},"notebookId":"666709baebd74c5c87d68191","trusted":true},"source":"!pip install jieba","outputs":[{"output_type":"stream","text":"Requirement already satisfied: jieba in /opt/conda/lib/python3.6/site-packages\n\u001b[33mYou are using pip version 9.0.1, however version 19.1.1 is available.\nYou should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n","name":"stdout"}],"execution_count":17},{"cell_type":"code","metadata":{"id":"6EAD841EBFE140C58516B2CA1AA489F9","collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"jupyter":{},"notebookId":"666709baebd74c5c87d68191","trusted":true},"source":"!pip install snownlp","outputs":[{"output_type":"stream","text":"Collecting snownlp\n  Downloading https://files.pythonhosted.org/packages/3d/b3/37567686662100d3bce62d3b0f2adec18ab4b9ff2b61abd7a61c39343c1d/snownlp-0.12.3.tar.gz (37.6MB)\n\u001b[K    100% |████████████████████████████████| 37.6MB 15kB/s \n\u001b[?25hBuilding wheels for collected packages: snownlp\n  Running setup.py bdist_wheel for snownlp ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \bdone\n\u001b[?25h  Stored in directory: /home/kesci/.cache/pip/wheels/f3/81/25/7c197493bd7daf177016f1a951c5c3a53b1c7e9339fd11ec8f\nSuccessfully built snownlp\nInstalling collected packages: snownlp\nSuccessfully installed snownlp-0.12.3\n\u001b[33mYou are using pip version 9.0.1, however version 19.1.1 is available.\nYou should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n","name":"stdout"}],"execution_count":18},{"cell_type":"code","metadata":{"id":"5459E32804FD4D0692CD3E89CA63D955","collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"jupyter":{},"notebookId":"666709baebd74c5c87d68191","trusted":true},"source":"!pip install textrank4zh","outputs":[{"output_type":"stream","text":"Requirement already satisfied: textrank4zh in /opt/conda/lib/python3.6/site-packages\nRequirement already satisfied: numpy>=1.7.1 in /opt/conda/lib/python3.6/site-packages (from textrank4zh)\nRequirement already satisfied: jieba>=0.35 in /opt/conda/lib/python3.6/site-packages (from textrank4zh)\nRequirement already satisfied: networkx>=1.9.1 in /opt/conda/lib/python3.6/site-packages (from textrank4zh)\nRequirement already satisfied: decorator>=4.1.0 in /opt/conda/lib/python3.6/site-packages (from networkx>=1.9.1->textrank4zh)\n\u001b[33mYou are using pip version 9.0.1, however version 19.1.1 is available.\nYou should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n","name":"stdout"}],"execution_count":19},{"cell_type":"markdown","metadata":{"id":"10346563EA614ABE897F7A8BCC0F61C9","mdEditEnable":false,"tags":[],"slideshow":{"slide_type":"slide"},"jupyter":{},"runtime":{"status":"default","execution_status":null,"is_visible":false},"scrolled":false,"notebookId":"666709baebd74c5c87d68191"},"source":"### 数据集介绍"},{"cell_type":"code","metadata":{"id":"B7B2A178532E41468A51C42AE7A0FEAD","collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"jupyter":{},"notebookId":"666709baebd74c5c87d68191","trusted":true},"source":"# 查看当前挂载的数据集目录\n!ls /home/kesci/work/xiaozhi","outputs":[{"output_type":"stream","text":"mydata.txt\t     stopword.txt\t  text_classification.zip\r\nmy_network_model.h5  text_classification\r\n","name":"stdout"}],"execution_count":20},{"cell_type":"markdown","metadata":{"id":"89A035084EA747638D3940F7D2EE3E8C","mdEditEnable":false,"tags":[],"slideshow":{"slide_type":"slide"},"jupyter":{},"runtime":{"status":"default","execution_status":null,"is_visible":false},"scrolled":false,"notebookId":"666709baebd74c5c87d68191"},"source":"* 本文的数据集 mydata.txt 介绍：  \n    * 1、这是小知同学本人在自己的头条号里面写的一篇文章，是关于爬虫学习路线的文章  \n    * 2、把文章复制保存在 mydata.txt 数据集中，为了方便来理解中文分词是如何切分任意一篇中文文章的  \n    * 3、数据集的原文链接：https://www.toutiao.com/i6611872532448412164/"},{"cell_type":"markdown","metadata":{"id":"79C6945B11914856B4B13DEA2867757B","mdEditEnable":false,"tags":[],"slideshow":{"slide_type":"slide"},"jupyter":{},"runtime":{"status":"default","execution_status":null,"is_visible":false},"scrolled":false,"notebookId":"666709baebd74c5c87d68191"},"source":"### 1、jieba 分词模块功能概要\r \n* -----------------------\r \n#### 第一块：分词模式和并行分词\r \n1. 精确分词\r \n试图将句子最精确地切开\r \n>jieba.cut('人生苦短，我学python',cut_all=False) \r\n\r\n2. 全模式\r \n把句子中所有的可能是词语的都扫描出来，速度非常快，但不能解决歧义\r \n>jieba.cut('人生苦短，我学python',cut_all=True) \r\n\r\n3. 搜索引擎模式\r \n在精确模式的基础上，对长词再次切分，提高召回率，适合用于搜索引擎分词\r \n>jieba.cut_for_search('人生苦短，我学python')\r \n\r\n4. 用 lcut 生成 list\r \n>jieba.lcut 对 cut 的结果做了封装，l 代表 list，即返回的结果是一个 list 集合  \r\njieba.lcut('人生苦短，我学python',cut_all=False) \r\n\r\n5. 原理：将目标文本按行分隔后，把各行文本分配到多个 Python 进程并行分词，\r \n然后归并结果，从而获得分词速度的可观提升.\r \n基于 python 自带的 multiprocessing 模块，目前暂不支持 Windows\r \n用法：\r \n>jieba.enable_parallel(4) # 开启并行分词模式，参数为并行进程数  \r\njieba.disable_parallel() # 关闭并行分词模式\r \n   \r\n* ------------------\r \n#### 第二块：关键词提取、词性标注、自定义字典\r \n6. 获取分词结果中词列表的 top n(关键词提取)\r \n>jieba.analyse.extract_tags(sentence, topK=20, withWeight=False, allowPOS=())    \r\njieba.analyse.textrank(sentence, topK=20, withWeight=False, allowPOS=())\r \n\r\n5. 获取词性\r \n>jieba.posseg 模块实现词性标注  \r\njieba.posseg.cut(sentence, cut_all=False)\r \n\r\n7. 自定义添加词和字典\r \n使用默认分词，是识别不出一句话中的新词，需要添加新词到字典\r \n\r\n自定义字典原理：\r \n* 词典格式和 dict.txt 一样，一个词占一行；每一行分三部分：\r \n* 词语、词频（可省略）、词性（可省略），用空格隔开，顺序不可颠倒\r \n* file_name 若为路径或二进制方式打开的文件，则文件必须为 UTF-8 编码 \r\n\r\n>D:/python/xiaozhi_life_dict.txt  \r\n小知同学  \r\n腾星火耀  \r\n绿杨楼  \r\n黄槐楼  \r\n尚书院  \r\n\r\n* jieba.load_userdict(file_name) # file_name 为文件类对象或自定义词典的路径\r \n\r\n完整示例\r \n>import jieba  \r\ntext = \"\"\"  \r\n小知同学时而在绿杨楼溜达，时而在黄槐楼遛狗，时而去尚书院阅读书籍，  \r\n时而去腾星火耀办事处。  \r\n\"\"\"  \r\njieba.load_userdict('D:/python/xiaozhi_life_dict.txt')  \r\ndata = jieba.cut(text)  # 默认精准模式  \r\nprint(','.join(data))  \r\n\r\n4、还可以做：\r \n自动摘要、依存句法分析、情感分析等任务"},{"cell_type":"markdown","metadata":{"id":"4C3E2C712C8449768EE1029923BBC5D5","mdEditEnable":false,"tags":[],"slideshow":{"slide_type":"slide"},"jupyter":{},"runtime":{"status":"default","execution_status":null,"is_visible":false},"scrolled":false,"notebookId":"666709baebd74c5c87d68191"},"source":"### 2、jieba 模块实战  \n#### 2.1 三种分词模式"},{"cell_type":"code","metadata":{"id":"DCE092CB3E1145CE97C71A4F116B809E","collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"jupyter":{},"notebookId":"666709baebd74c5c87d68191","trusted":true},"source":"# coding:utf-8\nimport jieba\n\nsentence = open('/home/kesci/work/xiaozhi/mydata.txt', 'rb').read()\n\njieba.enable_parallel(4)  # 开启并行分词\n# words = jieba.cut(sentence=sentence, cut_all=True)  # 全模式分词\nwords = jieba.cut(sentence=sentence, cut_all=False)  # 精确分词\n# words = jieba.cut_for_search(sentence=sentence, HMM=True)  # 搜索引擎分词\n\nprint('/'.join(words))\n","outputs":[{"output_type":"stream","text":"大家/都/知道/，/学习/一门/学科/的/时候/是/要/清楚/它/的/知识/框架/才能/清晰/的/学习/、/有/系统/的/学习/，/下面/来列/一列/python/网络/爬虫/的/知识/框架/来/帮助/大家/能够/有效/的/学习/和/掌握/，/避免/不必要/的/坑/。/\r\n/\r\n/python/网络/爬虫/总的来说/有/五个/大/的/方面/：/\r\n/前端/知识/—/—/基础/爬虫/—/—/框架/爬虫/—/—/分布式/爬虫/—/—/突破/反/爬虫/\r\n/\r\n/1/./前端/知识/：/\r\n/“/网络/爬虫/”/很/明显/对象/是/网络/，/也/就是/网页/。/说/到/网页/，/这里/就/涉及/到/了/前端/的/知识/了/，/不过/大家/也/不要/慌/，/只要/懂点/必要/的/HTML5/框架/、/网页/的/http/请求/、/还有/JavaScript/、/css3/的/知识/就/可以/了/，/以/这样/的/水平/也/是/可以/学会/爬虫/的/啦/。/当然/，/如果/要/非常/精通/python/网络/爬虫/的话/，/深入/学习/前端/知识/是/必要/的/。/\r\n/\r\n/2/./基础/爬虫/：/\r\n/（/1/）/基础/库/：/urllib/模块///requests/第三方/模块/\r\n/\r\n/首先/爬虫/就是/要/从/网页/上/把/我们/需要/的/信息/抓取/下来/的/，/那么/我们/就要/学习/urllib///requests/模块/，/这/两种/模块/是/负责/爬取/网页/的/。/这里/大家/觉得/哪/一种/用/的/习惯/就/用/哪/一种/，/选择/一种/精通/就/好/了/。/小编/推荐/读者/使用/使用/requests/模块/，/因为/这/一种/简便/很多/，/容易/操作/、/容易/理解/，/所以/requests/被/称为/“/人性化/模块/”/。/\r\n/\r\n/（/2/）/多/进程/、/多线程/和/协程/：/\r\n/\r\n/为什么/要学/着/三个/知识/呢/？/假如/你/要/爬/取/200/万条/的/数据/，/使用/一般/的/单/进程/或者/单线程/的话/，/你/爬/取/下载/这些/数据/，/也许/要/一个/星期/或是/更久/。/试问/这/是/你/想要/看到/的/结果/吗/？/显然/单/进程/和/单线程/不要/满足/我们/追求/的/高效率/，/太/浪费时间/了/。/只要/设置/好多/进程/和/多线程/，/爬取/数据/的/速度/可以/提高/10/倍/甚至/更/高/的/效率/。/\r\n/\r\n/（/3/）/网页/解析/提取/库/：/xpath///BeautifulSoup4///正则表达式/\r\n/\r\n/通过/前面/的/（/1/）/和/（/2/）/爬取/下来/的/是/网页/源代码/，/这里/有/很多/并/不是/我们/想要/的/信息/，/所以/需要/将/没用/的/信息/过滤/掉/，/留下/对/我们/有/价值/的/信息/。/这里/有/三种/解析器/，/三种/在/不同/的/场景/各有特色/也/各有/不足/，/总的来说/，/学会/这/三种/灵活运用/会/很/方便/的/。/推荐/理解能力/不是/很强/的/朋友/或是/刚/入门/爬虫/的/朋友/，/学习/BeautifulSoup4/是/很/容易/掌握/并/能够/快速/应用/实战/的/，/功能/也/非常/强大/。/\r\n/\r\n/（/4/）/反/屏蔽/：/请求/头///代理服务器///cookie/\r\n/\r\n/在/爬/取/网页/的/时候/有时/会/失败/，/因为/别人/网站/设置/了/反/爬虫/措施/了/，/这个/时候/就/需要/我们/去/伪装/自己/的/行为/，/让/对方/网站/察觉/不到/我们/就是/爬虫/方/。/请求/头/设置/，/主要/是/模拟/成/浏览器/的/行为/；/IP/被/屏蔽/了/，/就/需要/使用/代理服务器/来/破解/；/而/cookie/是/模拟/成/登录/的/行为/进入/网站/。/\r\n/\r\n/（/5/）/异常/：/超时/处理///异常/处理/，/这里/不/做/介绍/了/，/自己/去/了解/一下/。/\r\n/\r\n/（/6/）/数据/储存库/：/文件系统/储存///MySQL///MongoDB/\r\n/\r\n/数据/的/储存/大概/就/这/三种/方式/了/，/文件系统/储存/是/运用/了/python/文件/操作/来/执行/的/；/而/MySQL/要/使用/到/数据库/创建表格/来/储存/数据/；/MongoDB/在/爬虫/里/是/非常/好/的/储存/方式/，/分布式/爬虫/就是/运用/了/MongoDB/来/储存/的/。/各有特色/，/看/自己/需要/哪/种/，/在/灵活运用/。/\r\n/\r\n/（/7/）/动态/网页/抓取/：/Ajax///PhantomJS///Selenium/这/三个/知识点/\r\n/\r\n/（/8/）/抓包/：/APP/抓包///API/爬虫/\r\n/\r\n/3/./框架/爬虫/：/主流/且/热门/的/scrapy/框架///人性化/的/pyspider/框架/\r\n/框架/不止/这/两种/，/但是/很多/时候/就/只用/到/了/这些/框架/，/所以/把/这/两种/掌握/熟悉/了/就/可以/了/。/\r\n/\r\n/4/./分布式/爬虫/：/python/操作/Redis///scrapy/操作/Redis/\r\n/5/./突破/反/爬虫/：/useragent/池///禁用/cookies///设置/下载/延时/和/自动/限速///代理/IP/池///tor/代理///分布式/下载/器/\r\n/以上/就是/python/网络/爬虫/的/从/入门/到/精通/的/知识/框架/，/希望/这/篇文章/能/让/读者/高效/的/学好/python/网络/爬虫/。\n","name":"stdout"}],"execution_count":21},{"cell_type":"code","metadata":{"id":"3939C6E5DC99430C8AABEAD3280202CB","collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"jupyter":{},"notebookId":"666709baebd74c5c87d68191","trusted":true},"source":"","outputs":[],"execution_count":null},{"cell_type":"code","metadata":{"id":"AB0E9E671DA84A3D8DED3BD88443D513","collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"jupyter":{},"notebookId":"666709baebd74c5c87d68191","trusted":true},"source":"# 分词后以列表形式返回\nword_list = jieba.lcut(sentence=sentence)  # 默认精准分词\nprint(' '.join(word_list))","outputs":[{"output_type":"stream","text":"大家 都 知道 ， 学习 一门 学科 的 时候 是 要 清楚 它 的 知识 框架 才能 清晰 的 学习 、 有 系统 的 学习 ， 下面 来列 一列 python 网络 爬虫 的 知识 框架 来 帮助 大家 能够 有效 的 学习 和 掌握 ， 避免 不必要 的 坑 。 \r\n \r\n python 网络 爬虫 总的来说 有 五个 大 的 方面 ： \r\n 前端 知识 — — 基础 爬虫 — — 框架 爬虫 — — 分布式 爬虫 — — 突破 反 爬虫 \r\n \r\n 1 . 前端 知识 ： \r\n “ 网络 爬虫 ” 很 明显 对象 是 网络 ， 也 就是 网页 。 说 到 网页 ， 这里 就 涉及 到 了 前端 的 知识 了 ， 不过 大家 也 不要 慌 ， 只要 懂点 必要 的 HTML5 框架 、 网页 的 http 请求 、 还有 JavaScript 、 css3 的 知识 就 可以 了 ， 以 这样 的 水平 也 是 可以 学会 爬虫 的 啦 。 当然 ， 如果 要 非常 精通 python 网络 爬虫 的话 ， 深入 学习 前端 知识 是 必要 的 。 \r\n \r\n 2 . 基础 爬虫 ： \r\n （ 1 ） 基础 库 ： urllib 模块 / requests 第三方 模块 \r\n \r\n 首先 爬虫 就是 要 从 网页 上 把 我们 需要 的 信息 抓取 下来 的 ， 那么 我们 就要 学习 urllib / requests 模块 ， 这 两种 模块 是 负责 爬取 网页 的 。 这里 大家 觉得 哪 一种 用 的 习惯 就 用 哪 一种 ， 选择 一种 精通 就 好 了 。 小编 推荐 读者 使用 使用 requests 模块 ， 因为 这 一种 简便 很多 ， 容易 操作 、 容易 理解 ， 所以 requests 被 称为 “ 人性化 模块 ” 。 \r\n \r\n （ 2 ） 多 进程 、 多线程 和 协程 ： \r\n \r\n 为什么 要学 着 三个 知识 呢 ？ 假如 你 要 爬 取 200 万条 的 数据 ， 使用 一般 的 单 进程 或者 单线程 的话 ， 你 爬 取 下载 这些 数据 ， 也许 要 一个 星期 或是 更久 。 试问 这 是 你 想要 看到 的 结果 吗 ？ 显然 单 进程 和 单线程 不要 满足 我们 追求 的 高效率 ， 太 浪费时间 了 。 只要 设置 好多 进程 和 多线程 ， 爬取 数据 的 速度 可以 提高 10 倍 甚至 更 高 的 效率 。 \r\n \r\n （ 3 ） 网页 解析 提取 库 ： xpath / BeautifulSoup4 / 正则表达式 \r\n \r\n 通过 前面 的 （ 1 ） 和 （ 2 ） 爬取 下来 的 是 网页 源代码 ， 这里 有 很多 并 不是 我们 想要 的 信息 ， 所以 需要 将 没用 的 信息 过滤 掉 ， 留下 对 我们 有 价值 的 信息 。 这里 有 三种 解析器 ， 三种 在 不同 的 场景 各有特色 也 各有 不足 ， 总的来说 ， 学会 这 三种 灵活运用 会 很 方便 的 。 推荐 理解能力 不是 很强 的 朋友 或是 刚 入门 爬虫 的 朋友 ， 学习 BeautifulSoup4 是 很 容易 掌握 并 能够 快速 应用 实战 的 ， 功能 也 非常 强大 。 \r\n \r\n （ 4 ） 反 屏蔽 ： 请求 头 / 代理服务器 / cookie \r\n \r\n 在 爬 取 网页 的 时候 有时 会 失败 ， 因为 别人 网站 设置 了 反 爬虫 措施 了 ， 这个 时候 就 需要 我们 去 伪装 自己 的 行为 ， 让 对方 网站 察觉 不到 我们 就是 爬虫 方 。 请求 头 设置 ， 主要 是 模拟 成 浏览器 的 行为 ； IP 被 屏蔽 了 ， 就 需要 使用 代理服务器 来 破解 ； 而 cookie 是 模拟 成 登录 的 行为 进入 网站 。 \r\n \r\n （ 5 ） 异常 ： 超时 处理 / 异常 处理 ， 这里 不 做 介绍 了 ， 自己 去 了解 一下 。 \r\n \r\n （ 6 ） 数据 储存库 ： 文件系统 储存 / MySQL / MongoDB \r\n \r\n 数据 的 储存 大概 就 这 三种 方式 了 ， 文件系统 储存 是 运用 了 python 文件 操作 来 执行 的 ； 而 MySQL 要 使用 到 数据库 创建表格 来 储存 数据 ； MongoDB 在 爬虫 里 是 非常 好 的 储存 方式 ， 分布式 爬虫 就是 运用 了 MongoDB 来 储存 的 。 各有特色 ， 看 自己 需要 哪 种 ， 在 灵活运用 。 \r\n \r\n （ 7 ） 动态 网页 抓取 ： Ajax / PhantomJS / Selenium 这 三个 知识点 \r\n \r\n （ 8 ） 抓包 ： APP 抓包 / API 爬虫 \r\n \r\n 3 . 框架 爬虫 ： 主流 且 热门 的 scrapy 框架 / 人性化 的 pyspider 框架 \r\n 框架 不止 这 两种 ， 但是 很多 时候 就 只用 到 了 这些 框架 ， 所以 把 这 两种 掌握 熟悉 了 就 可以 了 。 \r\n \r\n 4 . 分布式 爬虫 ： python 操作 Redis / scrapy 操作 Redis \r\n 5 . 突破 反 爬虫 ： useragent 池 / 禁用 cookies / 设置 下载 延时 和 自动 限速 / 代理 IP 池 / tor 代理 / 分布式 下载 器 \r\n 以上 就是 python 网络 爬虫 的 从 入门 到 精通 的 知识 框架 ， 希望 这 篇文章 能 让 读者 高效 的 学好 python 网络 爬虫 。\n","name":"stdout"}],"execution_count":22},{"cell_type":"markdown","metadata":{"id":"8659093AA0194F1F83E083CDCAAA8784","mdEditEnable":false,"tags":[],"slideshow":{"slide_type":"slide"},"jupyter":{},"runtime":{"status":"default","execution_status":null,"is_visible":false},"scrolled":false,"notebookId":"666709baebd74c5c87d68191"},"source":"#### 2.2 我们来做文本分析，提取文本中的关键词  \n\n#### 通过提取出文档的关键词，我们可以了解到：  \n* 文章属于什么类型文体，如：科技、生活、健康、环保(广义)  \n* 文章大致说什么样的话题，如：科技类别中的某个知识点(狭义)  \n* 可以控制不同词性的输出，如：只输出人名(可以知道这篇文章讲到了哪个人)"},{"cell_type":"markdown","metadata":{"id":"35BBB70DB72A476D845BF520DB9D8D5D","mdEditEnable":false,"tags":[],"slideshow":{"slide_type":"slide"},"jupyter":{},"runtime":{"status":"default","execution_status":null,"is_visible":false},"scrolled":false,"notebookId":"666709baebd74c5c87d68191"},"source":"* 第一种文本分析方法  \n    * jieba.analyse.textrank()"},{"cell_type":"code","metadata":{"id":"F3901C5FFF0C44FEBF1E673A076528CB","collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"jupyter":{},"notebookId":"666709baebd74c5c87d68191","trusted":true},"source":"from jieba import analyse\n\nsentence = open('/home/kesci/work/xiaozhi/mydata.txt', 'rb').read()\nkey_words = analyse.textrank(sentence=sentence, topK=20, withWeight=True, \n                             allowPOS=('n'))\nkey_words","outputs":[{"output_type":"execute_result","metadata":{},"data":{"text/plain":"[('爬虫', 1.0),\n ('框架', 0.48032277009554275),\n ('网页', 0.375903778558585),\n ('储存', 0.28989294150612954),\n ('模块', 0.28396439713396376),\n ('网络', 0.26928056210536605),\n ('分布式', 0.25213862134805176),\n ('数据', 0.24921712051780848),\n ('学会', 0.18787346572729172),\n ('进程', 0.17506500529136249),\n ('高效率', 0.17506500529136249),\n ('信息', 0.17506500529136249),\n ('协程', 0.17381538416008774),\n ('浪费时间', 0.17381538416008774),\n ('价值', 0.17381538416008774),\n ('时候', 0.1577095931982778),\n ('网站', 0.14990211449063331),\n ('措施', 0.13718889861223857),\n ('方式', 0.1311555495988141),\n ('朋友', 0.12911459351451918)]"},"execution_count":23}],"execution_count":23},{"cell_type":"markdown","metadata":{"id":"E6043060F5084BA78B695D9AB822F3EC","mdEditEnable":false,"tags":[],"slideshow":{"slide_type":"slide"},"jupyter":{},"runtime":{"status":"default","execution_status":null,"is_visible":false},"scrolled":false,"notebookId":"666709baebd74c5c87d68191"},"source":"* 第二种文本分析方法  \n    * jieba.analyse.extract_tags()"},{"cell_type":"code","metadata":{"id":"E5C850CC819D4D718A34498787133EA0","collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"jupyter":{},"notebookId":"666709baebd74c5c87d68191","trusted":true},"source":"from jieba import analyse\n\nsentence = open('/home/kesci/work/xiaozhi/mydata.txt', 'rb').read()\n''' \n1、采用默认的情况下 \ntopK: 默认输出排名靠前的 20 个高频词(关键词)\nwithWeight: 是否一起输出词的权重，默认不输出权重\nallowPOS: 筛选什么样的词性输出，默认输出所有\n'''\nkey_words = analyse.extract_tags(sentence=sentence, topK=20, withWeight=False, \n                                 allowPOS=())\nkey_words","outputs":[{"output_type":"execute_result","metadata":{},"data":{"text/plain":"['爬虫',\n '网页',\n 'python',\n '框架',\n '知识',\n '模块',\n '储存',\n 'requests',\n '网络',\n '分布式',\n '学习',\n '爬取',\n 'MongoDB',\n '前端',\n '三种',\n '数据',\n '多线程',\n '代理服务器',\n '进程',\n '设置']"},"execution_count":24}],"execution_count":24},{"cell_type":"code","metadata":{"id":"B9989926055542EA83410F8C3774110C","collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"jupyter":{},"notebookId":"666709baebd74c5c87d68191","trusted":true},"source":"''' 2、只输出 10 个关键词 '''\nkey_words = analyse.extract_tags(sentence=sentence, topK=10)\nkey_words","outputs":[{"output_type":"execute_result","metadata":{},"data":{"text/plain":"['爬虫', '网页', 'python', '框架', '知识', '模块', '储存', 'requests', '网络', '分布式']"},"execution_count":25}],"execution_count":25},{"cell_type":"code","metadata":{"id":"30B90D668B4444E285F0AB84C217C34F","collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"jupyter":{},"notebookId":"666709baebd74c5c87d68191","trusted":true},"source":"''' 3、输出关键词的权重 '''\nkey_words = analyse.extract_tags(sentence=sentence, topK=10, withWeight=True)\nkey_words","outputs":[{"output_type":"execute_result","metadata":{},"data":{"text/plain":"[('爬虫', 0.576025142312844),\n ('网页', 0.19674556925594036),\n ('python', 0.19193434064288992),\n ('框架', 0.16611620656215598),\n ('知识', 0.1331185750895642),\n ('模块', 0.12328947563353213),\n ('储存', 0.11111941575137615),\n ('requests', 0.10967676608165138),\n ('网络', 0.09969344697511467),\n ('分布式', 0.09517722135229358)]"},"execution_count":26}],"execution_count":26},{"cell_type":"code","metadata":{"id":"94AB792BEF004A6B8B4AA0AC779DC5A6","collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"jupyter":{},"notebookId":"666709baebd74c5c87d68191","trusted":true},"source":"''' 4、设置词性，筛选出想要的关键词 '''\nkey_words = analyse.extract_tags(sentence=sentence, topK=10, withWeight=True, \n                                 allowPOS=('v'))  # 在这里我指定只提取动词的关键词\n# 有兴趣可以去试一试其他的词性\nkey_words\n","outputs":[{"output_type":"execute_result","metadata":{},"data":{"text/plain":"[('爬取', 0.703221617817647),\n ('知识', 0.5690166543044118),\n ('学习', 0.39646900704284316),\n ('使用', 0.22138725209754903),\n ('抓取', 0.22081608475294118),\n ('操作', 0.21603219419098038),\n ('需要', 0.2129647374264706),\n ('下载', 0.21024727281852942),\n ('请求', 0.2092673531461765),\n ('掌握', 0.18116407535117648)]"},"execution_count":27}],"execution_count":27},{"cell_type":"markdown","metadata":{"id":"10B4AF3F0E4F4E6692D17402C3A689AD","mdEditEnable":false,"tags":[],"slideshow":{"slide_type":"slide"},"jupyter":{},"runtime":{"status":"default","execution_status":null,"is_visible":false},"scrolled":false,"notebookId":"666709baebd74c5c87d68191"},"source":"#### 其他词性对应的字母如下：  \n* 该内容来源于网络  \n* --------------------------------  \n#### ICTCLAS 汉语词性标注集  \n#### 代码\t名称\t帮助记忆的诠释  \n* Ag\t形语素\t形容词性语素。形容词代码为a，语素代码ｇ前面置以A。  \n* a\t形容词\t取英语形容词adjective的第1个字母。  \n* ad\t副形词\t直接作状语的形容词。形容词代码a和副词代码d并在一起。  \n* an\t名形词\t具有名词功能的形容词。形容词代码a和名词代码n并在一起。  \n* b\t区别词\t取汉字“别”的声母。  \n* c\t连词\t取英语连词conjunction的第1个字母。  \n* Dg\t副语素\t副词性语素。副词代码为d，语素代码ｇ前面置以D。  \n* d\t副词\t取adverb的第2个字母，因其第1个字母已用于形容词。  \n* e\t叹词\t取英语叹词exclamation的第1个字母。  \n* f\t方位词\t取汉字“方” 的声母。  \n* g\t语素\t绝大多数语素都能作为合成词的“词根”，取汉字“根”的声母。  \n* h\t前接成分\t取英语head的第1个字母。  \n* i\t成语\t取英语成语idiom的第1个字母。  \n* j\t简称略语\t取汉字“简”的声母。  \n* k\t后接成分\t \n* l\t习用语\t习用语尚未成为成语，有点“临时性”，取“临”的声母。  \n* m\t数词\t取英语numeral的第3个字母，n，u已有他用。  \n* Ng\t名语素\t名词性语素。名词代码为n，语素代码ｇ前面置以N。  \n* n\t名词\t取英语名词noun的第1个字母。  \n* nr\t人名\t名词代码n和“人(ren)”的声母并在一起。  \n* ns\t地名\t名词代码n和处所词代码s并在一起。  \n* nt\t机构团体\t“团”的声母为t，名词代码n和t并在一起。  \n* nz\t其他专名\t“专”的声母的第1个字母为z，名词代码n和z并在一起。  \n* o\t拟声词\t取英语拟声词onomatopoeia的第1个字母。  \n* p\t介词\t取英语介词prepositional的第1个字母。  \n* q\t量词\t取英语quantity的第1个字母。  \n* r\t代词\t取英语代词pronoun的第2个字母,因p已用于介词。  \n* s\t处所词\t取英语space的第1个字母。  \n* Tg\t时语素\t时间词性语素。时间词代码为t,在语素的代码g前面置以T。  \n* t\t时间词\t取英语time的第1个字母。  \n* u\t助词\t取英语助词auxiliary 的第2个字母,因a已用于形容词。  \n* Vg\t动语素\t动词性语素。动词代码为v。在语素的代码g前面置以V。  \n* v\t动词\t取英语动词verb的第一个字母。  \n* vd\t副动词\t直接作状语的动词。动词和副词的代码并在一起。  \n* vn\t名动词\t指具有名词功能的动词。动词和名词的代码并在一起。  \n* w\t标点符号\t \n* x\t非语素字\t非语素字只是一个符号，字母x通常用于代表未知数、符号。  \n* y\t语气词\t取汉字“语”的声母。  \n* z\t状态词\t取汉字“状”的声母的前一个字母。"},{"cell_type":"markdown","metadata":{"id":"0E316FC4BD4C4880B83AE0208D9B8BE4","mdEditEnable":false,"tags":[],"slideshow":{"slide_type":"slide"},"jupyter":{},"runtime":{"status":"default","execution_status":null,"is_visible":false},"scrolled":false,"notebookId":"666709baebd74c5c87d68191"},"source":"#### 2.3 词性标注"},{"cell_type":"code","metadata":{"id":"FD60A2F471084D0DAB9B640F60ED34E2","collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"jupyter":{},"notebookId":"666709baebd74c5c87d68191","trusted":true},"source":"from jieba import posseg\n\nsentence = open('/home/kesci/work/xiaozhi/mydata.txt', 'rb').read()\n# print('文本解码>>>\\n', sentence.decode('gbk')[:89])\n\nwords = posseg.cut(sentence)\nword_flag, i = [], 0\nfor word, flag in words:\n    i += 1\n    word_flag.append((word, flag))\n    if i <= 10:\n        print(word, flag)\nprint(word_flag[:10])","outputs":[{"output_type":"stream","text":"大家 n\n都 d\n知道 v\n， x\n学习 v\n一门 m\n学科 n\n的 uj\n时候 n\n是 v\n[('大家', 'n'), ('都', 'd'), ('知道', 'v'), ('，', 'x'), ('学习', 'v'), ('一门', 'm'), ('学科', 'n'), ('的', 'uj'), ('时候', 'n'), ('是', 'v')]\n","name":"stdout"}],"execution_count":28},{"cell_type":"markdown","metadata":{"id":"D3F70F5935F944BC83610A691D410ACD","mdEditEnable":false,"tags":[],"slideshow":{"slide_type":"slide"},"jupyter":{},"runtime":{"status":"default","execution_status":null,"is_visible":false},"scrolled":false,"notebookId":"666709baebd74c5c87d68191"},"source":"### 3、textrank4zh 模块功能介绍和使用  \n* 自动从文本中抽取关键词、关键短语和文本摘要"},{"cell_type":"markdown","metadata":{"id":"73542E089A2D4CFC93CE8B40482BCCCC","mdEditEnable":false,"tags":[],"slideshow":{"slide_type":"slide"},"jupyter":{},"runtime":{"status":"default","execution_status":null,"is_visible":false},"scrolled":false,"notebookId":"666709baebd74c5c87d68191"},"source":"#### 3-1、抽取关键词、关键短语\r \n* 函数以及参数介绍\r \n* tr4w.get_keywords(20, word_min_len=1)                    # 提取关键字\r \n    >Signature: tr4w.get_keywords(num=6, word_min_len=1)  \r\n    Docstring:  \r\n    获取最重要的num个长度大于等于word_min_len的关键词。  \r\n    Return:  \r\n    关键词列表。  \r\n* tr4w.get_keyphrases(keywords_num=20, min_occur_num= 2)   # 提取关键短语\r \n    >Signature: tr4w.get_keyphrases(keywords_num=12, min_occur_num=2)  \r\n    Docstring:  \r\n    获取关键短语。  \r\n    获取 keywords_num 个关键词构造的可能出现的短语，要求这个短语在原文本中至少出现的次数为min_occur_num。  \r\n    Return:  \r\n    关键短语的列表。"},{"cell_type":"code","metadata":{"id":"350ADD21C76840058B41BB006753A148","collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"jupyter":{},"notebookId":"666709baebd74c5c87d68191","trusted":true},"source":"from textrank4zh import TextRank4Keyword \n\n''' 官方说法：py2 中 text 必须是 utf8 编码的 str 或者 unicode 对象，\n              py3 中必须是 utf8 编码的 bytes 或者 str 对象\n    实际运用：在本案例中，使用 'utf-8' 不能正常运行，而采用 'gbk' 编码可以正常运行，\n              所以大家看着情况灵活运用吧 \n'''\ntext = open('/home/kesci/work/xiaozhi/mydata.txt', 'r', encoding='gbk').read()\ntr4w = TextRank4Keyword()\ntr4w.analyze(text=text, lower=True, window=2)  # lower 是否将文本转换为小写\n\n# ------------------------------------------------------\nprint('提取关键词>>>\\n', '---' * 15)\nfor item in tr4w.get_keywords(num=10, word_min_len=1):\n    print(item)\n    print(item.word, item['weight'])  # 关键词、对应的权重(字典操作)\n\n# ------------------------------------------------------\nprint('\\n提取关键短语>>>\\n', '---' * 15)\nfor item in tr4w.get_keyphrases(keywords_num=20, min_occur_num=2):\n    print(item)                    # 关键短语\n","outputs":[{"output_type":"stream","text":"提取关键词>>>\n ---------------------------------------------\n{'word': '爬虫', 'weight': 0.03936331625458298}\n爬虫 0.03936331625458298\n{'word': '框架', 'weight': 0.025920263885343478}\n框架 0.025920263885343478\n{'word': '网页', 'weight': 0.025429356795653592}\n网页 0.025429356795653592\n{'word': '使用', 'weight': 0.018341125509380565}\n使用 0.018341125509380565\n{'word': '学习', 'weight': 0.016347304362054803}\n学习 0.016347304362054803\n{'word': '需要', 'weight': 0.01545757433752497}\n需要 0.01545757433752497\n{'word': '信息', 'weight': 0.014839375107394526}\n信息 0.014839375107394526\n{'word': '储存', 'weight': 0.012989611470711638}\n储存 0.012989611470711638\n{'word': '知识', 'weight': 0.01245963506364818}\n知识 0.01245963506364818\n{'word': 'python', 'weight': 0.012371394251488572}\npython 0.012371394251488572\n\n提取关键短语>>>\n ---------------------------------------------\n框架爬虫\n知识框架\n爬取网页\n","name":"stdout"}],"execution_count":29},{"cell_type":"markdown","metadata":{"id":"74908F92CA6A45D3974F2CA3AEACC1D8","mdEditEnable":false,"tags":[],"slideshow":{"slide_type":"slide"},"jupyter":{},"runtime":{"status":"default","execution_status":null,"is_visible":false},"scrolled":false,"notebookId":"666709baebd74c5c87d68191"},"source":"#### 3-2、抽取文本摘要  \n* 1、目前主流的文本摘要方式  \n* 一种是抽取式（extractive）  \n    * 概念：  \n        * 抽取式顾名思义，就是按照一定的权重，从原文中寻找跟中心思想最接近的一条或几条句子。  \n    * 效果：  \n        * 抽取式的摘要目前已经比较成熟，但是抽取质量及内容流畅度不是那么理想  \n\n* 另一种是生成式（abstractive）  \n    * 概念：  \n        * 生成式则是计算机通读原文后，在理解整篇文章意思的基础上，按自己的话生成流畅的翻译。  \n    * 效果：  \n        * 伴随着深度学习的研究，生成式摘要的质量和流畅度都有很大的提升，  \n        * 但目前也受到原文本长度过长、抽取内容不佳等的限制。  \n        \n* -------------------  \n本案例使用抽取式方法  \n* 2、函数以及参数介绍  \n* tr4s.get_key_sentences(num=3)  \n    >Signature: tr4s.get_key_sentences(num=6, sentence_min_len=6)  \n    Docstring:  \n    获取最重要的num个长度大于等于sentence_min_len的句子用来生成摘要。  \n    "},{"cell_type":"code","metadata":{"id":"4B6C2D31D9D840C28C4D195FB68A3791","collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"jupyter":{},"notebookId":"666709baebd74c5c87d68191","trusted":true},"source":"from textrank4zh import TextRank4Sentence\n\ntext = open('/home/kesci/work/xiaozhi/mydata.txt', 'r', encoding='gbk').read()\ntr4s = TextRank4Sentence()\ntr4s.analyze(text=text, lower=True, source='all_filters') \n'''\nsource ——> 生成句子之间的相似度,默认值为`'all_filters'`，\n            可选值为`'no_filter', 'no_stop_words'\n'''\n \n# ------------------------------------------------\nprint('提取文本摘要>>>\\n', '---' * 20)\nfor item in tr4s.get_key_sentences(num=3):\n    print(item)\n    print(item.index, '\\t', item.weight, '\\t', item['sentence'], '\\n')","outputs":[{"output_type":"stream","text":"提取文本摘要>>>\n ------------------------------------------------------------\n{'index': 23, 'sentence': '在爬取网页的时候有时会失败，因为别人网站设置了反爬虫措施了，这个时候就需要我们去伪装自己的行为，让对方网站察觉不到我们就是爬虫方', 'weight': 0.04319642085085014}\n23 \t 0.04319642085085014 \t 在爬取网页的时候有时会失败，因为别人网站设置了反爬虫措施了，这个时候就需要我们去伪装自己的行为，让对方网站察觉不到我们就是爬虫方 \n\n{'index': 9, 'sentence': '首先爬虫就是要从网页上把我们需要的信息抓取下来的，那么我们就要学习urllib/requests模块，这两种模块是负责爬取网页的', 'weight': 0.04222076414544854}\n9 \t 0.04222076414544854 \t 首先爬虫就是要从网页上把我们需要的信息抓取下来的，那么我们就要学习urllib/requests模块，这两种模块是负责爬取网页的 \n\n{'index': 0, 'sentence': '大家都知道，学习一门学科的时候是要清楚它的知识框架才能清晰的学习、有系统的学习，下面来列一列python网络爬虫的知识框架来帮助大家能够有效的学习和掌握，避免不必要的坑', 'weight': 0.04108120739367898}\n0 \t 0.04108120739367898 \t 大家都知道，学习一门学科的时候是要清楚它的知识框架才能清晰的学习、有系统的学习，下面来列一列python网络爬虫的知识框架来帮助大家能够有效的学习和掌握，避免不必要的坑 \n\n","name":"stdout"}],"execution_count":30},{"cell_type":"markdown","metadata":{"id":"C31DA010FAA044F6A62D7FC201CE6312","mdEditEnable":false,"tags":[],"slideshow":{"slide_type":"slide"},"jupyter":{},"runtime":{"status":"default","execution_status":null,"is_visible":false},"scrolled":false,"notebookId":"666709baebd74c5c87d68191"},"source":"### 4、snownlp 模块  \n#### 4.1 snownlp 模块功能概要"},{"cell_type":"code","metadata":{"id":"C43AD81F9DB949E888396F03833DD0AD","collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"jupyter":{},"notebookId":"666709baebd74c5c87d68191","trusted":true},"source":"# ------------------------------------------------------\r\n# 第一块：总览其功能\r\n# from snownlp import SnowNLP  \r\n# s = SnowNLP()\r\n# SnowNLP类：  \r\n# s.words               # 功能1 分词  \r\n# s.pinyin              # 功能2 汉字转拼音 \r\n# s.han                 # 功能3 繁体转简体\r\n# s.tags                # 功能4 词性 \r\n# s.sentences           # 功能5 分句  \r\n# s.sentiments          # 功能6 情感度  \r\n# s.keywords(limit=10)  # 功能7 关键词  \r\n# s.summary(10          # 功能8 文本摘要\r\n# s.sim()               # 功能9 相似度分析  \r\n# s.tf                  # 功能10 tf = 某个词在本文档中出现的次数/本文档的总词数   \r\n# s.idf                 # 功能10 idf = log(语料库文档总数/(包含该词的文档数+1))  \r\n","outputs":[],"execution_count":31},{"cell_type":"markdown","metadata":{"id":"A104254197FB481F81AE24F833966198","mdEditEnable":false,"tags":[],"slideshow":{"slide_type":"slide"},"jupyter":{},"runtime":{"status":"default","execution_status":null,"is_visible":false},"scrolled":false,"notebookId":"666709baebd74c5c87d68191"},"source":"#### 4.2 snownlp 实战训练"},{"cell_type":"code","metadata":{"id":"6A1392A22E5D4DFFB851A6E2F770BAF0","collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"jupyter":{},"notebookId":"666709baebd74c5c87d68191","trusted":true},"source":"from snownlp import SnowNLP\n\n# 读取任意一篇中文文档\ntext = open('/home/kesci/work/xiaozhi/mydata.txt', encoding='gbk').read()\ns = SnowNLP(text)\n","outputs":[],"execution_count":32},{"cell_type":"code","metadata":{"id":"A7C5E9FCB2D84011A62528A0557025A7","collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"jupyter":{},"notebookId":"666709baebd74c5c87d68191","trusted":true},"source":"# 1、分词(效果不够 jieba 的分词效果好)\ns.words[:20]","outputs":[{"output_type":"execute_result","metadata":{},"data":{"text/plain":"['大家',\n '都',\n '知道',\n '，',\n '学习',\n '一',\n '门',\n '学科',\n '的',\n '时候',\n '是',\n '要',\n '清楚',\n '它',\n '的',\n '知识',\n '框架',\n '才',\n '能',\n '清晰']"},"execution_count":33}],"execution_count":33},{"cell_type":"code","metadata":{"id":"E3D01DB84E1F4D0D84D8BCD4E0078F3C","collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"jupyter":{},"notebookId":"666709baebd74c5c87d68191","trusted":true},"source":"# 2、汉字转拼音 \ns.pinyin[:20]","outputs":[{"output_type":"execute_result","metadata":{},"data":{"text/plain":"['da',\n 'jia',\n 'dou',\n 'zhi',\n 'dao',\n '，',\n 'xue',\n 'xi',\n 'yi',\n 'men',\n 'xue',\n 'ke',\n 'de',\n 'shi',\n 'hou',\n 'shi',\n 'yao',\n 'qing',\n 'chu',\n 'ta']"},"execution_count":34}],"execution_count":34},{"cell_type":"code","metadata":{"id":"ED226183845D4C1283760579147BEF2D","collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"jupyter":{},"notebookId":"666709baebd74c5c87d68191","trusted":true},"source":"# 3、繁体转简体\nfj = SnowNLP(u'「繁體字」「繁體中文」的叫法在臺灣亦很常見。')  \nfj.han ","outputs":[{"output_type":"execute_result","metadata":{},"data":{"text/plain":"'「繁体字」「繁体中文」的叫法在台湾亦很常见。'"},"execution_count":35}],"execution_count":35},{"cell_type":"code","metadata":{"id":"3661A19A5F5C457EA2DC04850AEECBDE","collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"jupyter":{},"notebookId":"666709baebd74c5c87d68191","trusted":true},"source":"# 4、词性标注(效果不够 jieba 的效果好)  \nk = 0\nfor i in s.tags:\n    k += 1\n    if k <= 20:\n        print(i)","outputs":[{"output_type":"stream","text":"('大家', 'r')\n('都', 'd')\n('知道', 'v')\n('，', 'w')\n('学习', 'v')\n('一', 'm')\n('门', 'q')\n('学科', 'n')\n('的', 'u')\n('时候', 'n')\n('是', 'v')\n('要', 'v')\n('清楚', 'a')\n('它', 'r')\n('的', 'u')\n('知识', 'n')\n('框架', 'n')\n('才', 'd')\n('能', 'v')\n('清晰', 'a')\n","name":"stdout"}],"execution_count":36},{"cell_type":"code","metadata":{"id":"BE2EC2305671458A85102774275955CB","collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"jupyter":{},"notebookId":"666709baebd74c5c87d68191","trusted":true},"source":"# 5、分句(在停用词逗号、句号等分隔符处分句)  \r\nsentence = s.sentences[:20]     \r\nsentence","outputs":[{"output_type":"execute_result","metadata":{},"data":{"text/plain":"['大家都知道',\n '学习一门学科的时候是要清楚它的知识框架才能清晰的学习、有系统的学习',\n '下面来列一列python网络爬虫的知识框架来帮助大家能够有效的学习和掌握',\n '避免不必要的坑',\n 'python网络爬虫总的来说有五个大的方面：',\n '前端知识——基础爬虫——框架爬虫——分布式爬虫——突破反爬虫',\n '1.前端知识：',\n '“网络爬虫”很明显对象是网络',\n '也就是网页',\n '说到网页',\n '这里就涉及到了前端的知识了',\n '不过大家也不要慌',\n '只要懂点必要的HTML5框架、网页的http请求、还有JavaScript、css3的知识就可以了',\n '以这样的水平也是可以学会爬虫的啦',\n '当然',\n '如果要非常精通python网络爬虫的话',\n '深入学习前端知识是必要的',\n '2.基础爬虫：',\n '（1）基础库：urllib模块/requests第三方模块',\n '首先爬虫就是要从网页上把我们需要的信息抓取下来的']"},"execution_count":37}],"execution_count":37},{"cell_type":"code","metadata":{"id":"F364F4B723124BDF95D283F68C4BF3E9","collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"jupyter":{},"notebookId":"666709baebd74c5c87d68191","trusted":true},"source":"# 6、情感分析(注意目前 snownlp 模块只针对买卖东西时的评价，其他方面的效果不好)  \n'''s = SnowNLP('这个商品很好用')  \ns.sentiments             # 0.7492624097842814 积极的概率(一般大于0.5)  \ns = SnowNLP('这个商品很垃圾')  \ns.sentiments             # 0.18571145823186264 消极的概率(一般小于0.5)\n'''\n\n# 说明：这篇文章不是买卖东西时的评论语来的，所以预测效果会不好的\n# 这里是为了告诉读者，它这个方法是怎样使用的\nfor item in sentence:\n    print(SnowNLP(item).sentiments)","outputs":[{"output_type":"stream","text":"0.5512110016420362\n0.9996878003759021\n0.9756245505565548\n0.05348175336301941\n0.4263989249763027\n0.9999719421657834\n0.9030141393768442\n0.7967692495130689\n0.3999999999999998\n0.27766712995735\n0.9798495749719074\n0.34527875471283087\n0.9444096163288037\n0.7388876001043244\n0.5262327818078083\n0.5688291481104605\n0.9866935756029875\n0.6294192655653609\n0.10200483072227873\n0.23933045688083876\n","name":"stdout"}],"execution_count":38},{"cell_type":"code","metadata":{"id":"D4E1FDADECC147329A4D091ECD5659D3","collapsed":false,"scrolled":false,"mdEditEnable":false,"tags":[],"slideshow":{"slide_type":"slide"},"jupyter":{},"notebookId":"666709baebd74c5c87d68191","trusted":true},"source":"# 7、提取关键词(效果不够 jieba 的效果好) \ns.keywords(20)","outputs":[{"output_type":"execute_result","metadata":{},"data":{"text/plain":"['爬虫',\n '网页',\n '不',\n '框架',\n '种',\n '知识',\n '储存',\n '数据',\n '模',\n '三',\n '式',\n '很',\n '库',\n 'python',\n '信息',\n '代理',\n '块',\n '设置',\n '学习',\n '网络']"},"execution_count":39}],"execution_count":39},{"cell_type":"code","metadata":{"id":"253AD9AAADD546CC84CC1911A7801EFF","collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"jupyter":{},"notebookId":"666709baebd74c5c87d68191","trusted":true},"source":"# 8、抽取文本摘要\ns.summary(20)","outputs":[{"output_type":"execute_result","metadata":{},"data":{"text/plain":"['以上就是python网络爬虫的从入门到精通的知识框架',\n '下面来列一列python网络爬虫的知识框架来帮助大家能够有效的学习和掌握',\n '这两种模块是负责爬取网页的',\n '框架不止这两种',\n '首先爬虫就是要从网页上把我们需要的信息抓取下来的',\n '数据的储存大概就这三种方式了',\n '前端知识——基础爬虫——框架爬虫——分布式爬虫——突破反爬虫',\n '学会这三种灵活运用会很方便的',\n '这里有很多并不是我们想要的信息',\n '如果要非常精通python网络爬虫的话',\n '三种在不同的场景各有特色也各有不足',\n '分布式爬虫就是运用了MongoDB来储存的',\n '让对方网站察觉不到我们就是爬虫方',\n '深入学习前端知识是必要的',\n '“网络爬虫”很明显对象是网络',\n 'MongoDB在爬虫里是非常好的储存方式',\n '只要懂点必要的HTML5框架、网页的http请求、还有JavaScript、css3的知识就可以了',\n '显然单进程和单线程不要满足我们追求的高效率',\n 'python网络爬虫总的来说有五个大的方面：',\n '这里有三种解析器']"},"execution_count":40}],"execution_count":40},{"cell_type":"code","metadata":{"id":"6BBA19E41D514EAC830251971C26A4FB","collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"jupyter":{},"notebookId":"666709baebd74c5c87d68191","trusted":true},"source":"# 9、相似度分析  \ns_sim = SnowNLP([\n                 [u'这篇', u'文章', '写作'],  \n                 [u'那篇', u'论文', '学术'],  \n                 [u'这个', '小知同学', '小知同学']\n                ])\n           \ns_sim.sim([u'小知同学'])        \n","outputs":[{"output_type":"execute_result","metadata":{},"data":{"text/plain":"[0, 0, 0.7297508910942724]"},"execution_count":41}],"execution_count":41},{"cell_type":"code","metadata":{"id":"C8A9A655E2834AF094EC17EE417CB6A2","collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"jupyter":{},"notebookId":"666709baebd74c5c87d68191","trusted":true},"source":"# 10、tf + idf\ns_sim.tf","outputs":[{"output_type":"execute_result","metadata":{},"data":{"text/plain":"[{'这篇': 1, '文章': 1, '写作': 1},\n {'那篇': 1, '论文': 1, '学术': 1},\n {'这个': 1, '小知同学': 2}]"},"execution_count":42}],"execution_count":42},{"cell_type":"code","metadata":{"id":"35E606D38856416981C1A5350A65C60A","collapsed":false,"scrolled":false,"mdEditEnable":false,"tags":[],"slideshow":{"slide_type":"slide"},"jupyter":{},"notebookId":"666709baebd74c5c87d68191","trusted":true},"source":"s_sim.idf","outputs":[{"output_type":"execute_result","metadata":{},"data":{"text/plain":"{'这篇': 0.5108256237659907,\n '文章': 0.5108256237659907,\n '写作': 0.5108256237659907,\n '那篇': 0.5108256237659907,\n '论文': 0.5108256237659907,\n '学术': 0.5108256237659907,\n '这个': 0.5108256237659907,\n '小知同学': 0.5108256237659907}"},"execution_count":43}],"execution_count":43},{"cell_type":"markdown","metadata":{"id":"3EA3DDFF0BB449948E8DFB5DD3331D1C","scrolled":false,"mdEditEnable":false,"tags":[],"slideshow":{"slide_type":"slide"},"jupyter":{},"runtime":{"status":"default","execution_status":null,"is_visible":false},"notebookId":"666709baebd74c5c87d68191"},"source":"### 5、参考文献"},{"cell_type":"markdown","metadata":{"id":"F29D377F9FDD445892091C615224DB23","mdEditEnable":false,"tags":[],"slideshow":{"slide_type":"slide"},"jupyter":{},"runtime":{"status":"default","execution_status":null,"is_visible":false},"scrolled":false,"notebookId":"666709baebd74c5c87d68191"},"source":"jieba模块相关：  \n>官网文档：https://github.com/fxsjy/jieba  \n\ntextrank4zh 模块相关：  \n>官网文档：https://github.com/letiantian/TextRank4ZH  \n\nsnownlp 模块相关：  \n>官网文档：https://github.com/isnowfy/snownlp  \n\n文本摘要相关：  \n>https://blog.csdn.net/qq_41853758/article/details/82859654"},{"cell_type":"code","metadata":{"id":"51836410F5DA42788A101FDE5C67DB59","collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"jupyter":{},"notebookId":"666709baebd74c5c87d68191","trusted":true},"source":"","outputs":[],"execution_count":null}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":0}